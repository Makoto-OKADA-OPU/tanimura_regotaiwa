{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vxgIAIRFd1Gls4pzJVrKLU9ruzUhQ_LT",
      "authorship_tag": "ABX9TyNU4c+8m4/+z0B79G5A4Zmt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Makoto-OKADA-OPU/tanimura_regotaiwa/blob/main/predictMaskedPoses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 日本語母語話者の英語対訳コーパスに対する深層言語モデルを用いた単語予測の分析と評価\n",
        "\n",
        "岡田 真\n",
        "\n",
        "修正：2024.07  \n",
        "初出：2023.10\n"
      ],
      "metadata": {
        "id": "-Rz1QcaJXR4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 概説\n",
        "「インタラクションと対話」[^1] 第 4 章 (岡田担当章) で用いた\n",
        "実験用 Python スクリプトです．  \n",
        "\n",
        "具体的には課題達成対話データから作成された文字化データに含まれる「ポーズ (沈黙)」箇所を深層言語モデルにより推定して分析しています．  \n",
        "\n",
        "その際の Python スクリプトになります．\n",
        "\n",
        "スクリプトの各部に説明を加えています．そちらを読みながら動作確認できます．\n",
        "\n",
        "[^1]:[谷村緑，仲本康一郎，吉田悦子：インタラクションと対話，開拓社，2024．](https://www.kaitakusha.co.jp/book/book.php?c=2401)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rwnx5Kq2KNEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 初期化\n",
        "ライブラリの読み込みやインストールをします．"
      ],
      "metadata": {
        "id": "NACmOigPTVQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p_G6ZDOAuxI"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "#-*- coding:utf-8 -*-\n",
        "\n",
        "# 必要なモジュールの読み込み\n",
        "import os # オペレーティングシステム (OS) インターフェース Python から OS にアクセスするためのライブラリ\n",
        "import glob # Unix 形式のパス名のパターン解析用ライブラリ Python からファイルやディレクトリにアクセスするためのライブラリ\n",
        "import pandas as pd # データ解析ライブラリ\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT を使う準備\n",
        "# Google Colaboratory で transformers ライブラリをインストールして tokenizer と model の用意をする．\n",
        "!pip install transformers # transformers のインストール\n",
        "\n",
        "import torch # PyTorch 深層学習用ライブラリの読み込み\n",
        "from transformers import BertTokenizer, BertForPreTraining # BERT のトークナイザと事前学習モデルの読み込み\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "mY3xEbSmOa6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 関数\n",
        "処理を簡単にするために関数を作成しています．"
      ],
      "metadata": {
        "id": "dAIwx773TIct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [MASK] で推測された上位の単語を得る関数\n",
        "# tokens: トークン\n",
        "# k: 上位 k 個取得 (デフォルト 20 個)\n",
        "def predictMaskedWord(tokens, k=20):\n",
        "  predicted_tokens = None\n",
        "  results_tokens = []\n",
        "\n",
        "  # '[MASK]' トークンが含まれるデータで予測する\n",
        "  if '[MASK]' in tokens: # [MASK] トークンを含んでいる場合に予測する\n",
        "    # [MASK] トークンの位置を見つけてその番号を保存する\n",
        "    # 1 つの文に複数の [MASK] トークンが含まれている場合があるので，\n",
        "    # 文中のすべての [MASK] トークンの位置の番号を保存する\n",
        "    masked_indecis = [i for i, x in enumerate(tokens) if x == '[MASK]']\n",
        "\n",
        "    # BERT で予測する\n",
        "    # トークン ID を取得して，BERT で処理をする形式に変更してから BERT にデータを入力する．\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    ids = torch.tensor(ids).reshape(1, -1) # バッチサイズ 1 の形に変形する\n",
        "\n",
        "    # BERT に入力して各トークンの分散表現ベクトルを得る\n",
        "    with torch.no_grad():\n",
        "      output = model(ids)\n",
        "\n",
        "    # 出力結果を検証する\n",
        "    # prediction_logits は [文の数，文中のトークンの数，全ての語彙の数] という形式のテンソルになっている\n",
        "    # 各文の各トークンに対してすべての語彙との距離が格納されている\n",
        "    # つまり各文の各単語について，全てのトークンとの分散表現ベクトルの距離の近さが取れる\n",
        "\n",
        "    pred = output.prediction_logits # masked language modeling の出力結果 (分散表現ベクトル)\n",
        "    pred = pred[0] # [文の数 (ここでは 1), トークンの数, 全ての語彙の数] という形なので最初の [1] を取り除くために代入し直す\n",
        "\n",
        "    # すべての [MASK] トークンについて他の全てのトークンとの距離のうち\n",
        "    # 上位から k 個のトークンの情報を得る\n",
        "    # torch.topk モジュールは入力されたテンソルの上位 k 個についてその値と ID を返す\n",
        "    # ここでは値は必要ないので _ で無視して，ID だけを得ている\n",
        "    for i in masked_indecis:\n",
        "      _, pred_idxs = torch.topk(pred[i], k) # 上位 k 個の ID を得る\n",
        "      predicted_tokens = tokenizer.convert_ids_to_tokens(pred_idxs.tolist()) # ID からトークン (文字列) を得る\n",
        "      results_tokens.append(predicted_tokens) # トークンを格納する\n",
        "\n",
        "  return results_tokens"
      ],
      "metadata": {
        "id": "yOHYCMs7vDud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 取得した予測トークンの集計をする関数\n",
        "# リスト中の全てのトークンを走査して，その出現頻度を数え上げる\n",
        "# listlisttokens トークンのリストのリスト\n",
        "# cnt トークンの出現頻度\n",
        "from collections import Counter\n",
        "\n",
        "def countMaskedWords(listlisttokens, cnt):\n",
        "  for listtoken in listlisttokens:\n",
        "    for token in listtoken:\n",
        "      cnt[token] += 1\n"
      ],
      "metadata": {
        "id": "KwMjYJpvx9Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 処理本体\n",
        "プログラム本体です．文字化データのファイルを読み込んで，[MASK] 部分に入るトークンを推定します．  \n"
      ],
      "metadata": {
        "id": "jWvXJt2aTQOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "文字化データを Google Drive 上に置いて，そこから読み込む場合，事前にデータを Google Drive 上に格納しておき，下記のコードを実行してプログラム中で自分の Google Drive 上のフォルダにアクセスできるようにする必要があります．  \n",
        "下記のコードを実行すると Google Drive へのアクセスを許可するかどうか聞かれますので，許可をして処理を進めます．"
      ],
      "metadata": {
        "id": "2dWgDnkeVqPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive をプログラム中から読み込めるようにする (マウントする)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cGmTTzVPXj2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "処理したい文字化データのファイルリストを得ます．  \n",
        "変数 ```filepath``` には文字化データのある Google Drive のディレクトリの完全パスを設定してください．  \n",
        "例えば Google Drive の MyDrive に mojikadata というディレクトリがあって，その中にあるのであれば，```filepath``` には ```\"/content/drive/MyDrive/mojikadata/\"``` という完全パスを入力します．  \n",
        "\n",
        "このプログラムでは，文字化データは Excel 形式のファイルという前提で作成しています．\n",
        "\n",
        "今回はサンプルデータを Google Drive の Colab Notebooks の中に WordEstimationUsingMaskedLearningModel というディレクトリを作り，この Colab Notebook を保存して，その同じディレクトリに sampledata というディレクトリを作って，そこに \"sampledata.xlsx\" という Excel 形式のサンプルファイルを置いています．  \n",
        "このファイルには書籍で例として使ったデータ 2 件が入っています．  \n",
        "\n",
        "実際に自分でこの Notebook を使う場合はデータのあるディレクトリの完全パスを自分の環境に合わせた設定に変更してください．"
      ],
      "metadata": {
        "id": "1Lueo7ehMsA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ファイルリストを得ます．\n",
        "# 具体的には対象となる文字化データファイルを探してその完全パスをすべて保存します．\n",
        "#\n",
        "# 今回はサンプルデータを Google Drive の Colab Notebooks の中に\n",
        "# WordEstimationUsingMaskedLearningModel というディレクトリを作り，\n",
        "# この Colab Notebook を保存して，その同じディレクトリに sampledata という\n",
        "# ディレクトリを作って，そこに \"sampledata.xlsx\" という Excel 形式の\n",
        "# サンプルファイルを置いています．\n",
        "# このファイルには書籍で例として使ったデータ 2 件が入っています．\n",
        "# 実際に自分で使う場合は自分の環境に合わせた設定に変更してください．\n",
        "filepath = \"/content/drive/MyDrive/Colab Notebooks/WordEstimationUsingMaskedLearningModel/sampledata/\"\n",
        "\n",
        "# ファイルパス内の該当ファイルを取得\n",
        "import glob\n",
        "files = glob.glob(filepath + '**/*.xlsx', recursive=True) # ディレクトリの中を\n",
        "files.sort()\n",
        "for f in files:\n",
        "  print(f) # 読み込んだデータファイルの確認"
      ],
      "metadata": {
        "id": "xev74wCfTdD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "文字化データが Excel 形式で格納されている前提で，今回は pandas という Python のデータ構造やデータ解析のライブラリを使って文字化データの読み込みや [MASK] 部分の推定とその推定結果の集計をしていきます．"
      ],
      "metadata": {
        "id": "nLZusgivm_Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データファイルで「ポーズ (沈黙)」部分を [MASK] して推測する\n",
        "# ライブラリの読み込み\n",
        "from tqdm.auto import tqdm # 処理の進行状況表示用ライブラリ\n",
        "import numpy as np # 数値計算用ライブラリ\n",
        "from collections import Counter, defaultdict # 汎用ユーティリティライブラリからトークンカウント用にモジュールを読み込む\n",
        "\n",
        "# トークンカウントの準備\n",
        "cnt = Counter()\n",
        "l_n_sentence = defaultdict(lambda:0)\n",
        "l_n_masked_sentence = defaultdict(lambda:0)\n",
        "\n",
        "\n",
        "for f in tqdm(files): # ファイルごとに処理\n",
        "  print(f) # 処理するファイル名を表示\n",
        "  basefilename, ext = os.path.splitext(os.path.basename(f))\n",
        "\n",
        "  # pandas.read_excel モジュールを使って読み込んでデータフレームに格納する\n",
        "  # 先頭行をヘッダとしないために header オプションに None を渡している\n",
        "  # これにより読み込んだ最初の行から使われて，0, 1, 2,... と番号付けられる\n",
        "  # Excel のシートごとに処理をするために sheet_name オプションを使う\n",
        "  # ここではすべてのシートを扱うために None を渡している\n",
        "  df = pd.read_excel(f, header=None, sheet_name=None)\n",
        "\n",
        "  # Excel のシートごとに処理する\n",
        "  for k in df.keys():\n",
        "    # 「ポーズ (沈黙)」部分を '[MASK]' で置換する．\n",
        "    # ファイルのデータは df に格納される．\n",
        "    # 0 列目は発話者の情報など，1 列目に発話内容の文字化データが格納されている\n",
        "    import re\n",
        "    df[k][2] = df[k][1].map(lambda x: re.sub(re.compile(r\"\\([0-9]+\\)\"), \"[MASK]\", x), na_action='ignore') # 文字化データに「ポーズ」があれば [MASK] に置き換えて，2 列目に格納する\n",
        "    df[k][3] = df[k][2].map(tokenizer.tokenize, na_action='ignore') # 2 列目のデータをトークンに分割して，3 列目に格納する\n",
        "\n",
        "    # 置換してトークン化したデータで Masked Word Prediction をして，予測単語の上位 20 語を得て，4 列目に保存する\n",
        "    df[k][4] = df[k][3].map(predictMaskedWord, na_action='ignore')\n",
        "\n",
        "    # 結果を集計する\n",
        "    # 文字化データの行数を数える\n",
        "    l_n_sentence[basefilename] = len(df[k][1].dropna())\n",
        "\n",
        "    # 予測単語を集計する\n",
        "    for x in df[k][4].dropna(how='all').to_numpy():\n",
        "      if len(x) > 0:\n",
        "        l_n_masked_sentence[basefilename] += 1\n",
        "        countMaskedWords(x, cnt)\n",
        "\n",
        "# 結果をファイルに出力する\n",
        "# シートごとに結果を出力していく\n",
        "with pd.ExcelWriter(filepath+basefilename+'.pred'+ext) as writer:\n",
        "  for k in df.keys():\n",
        "    Writer = df[k].to_excel(writer, index=False, header=False, sheet_name=k)\n",
        "\n",
        "# 集計結果を画面に出力して確認する\n",
        "for k, v in cnt.items():\n",
        "  print(k,v)\n"
      ],
      "metadata": {
        "id": "cvOkMdaWvtTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 結果を画面に出力して確認する\n",
        "# すべての文について\n",
        "print('Total Sentence')\n",
        "sum = 0\n",
        "for k, v in l_n_sentence.items():\n",
        "  print(k, v)\n",
        "  sum += v\n",
        "print(\"total: {}\".format(sum))\n",
        "\n",
        "# すべての [MASK] を含む文について\n",
        "print('Total Sentance with [MASK]')\n",
        "sum = 0\n",
        "for k, v in l_n_masked_sentence.items():\n",
        "  print(k, v)\n",
        "  sum += v\n",
        "print(\"total: {}\".format(sum))\n",
        "\n",
        "# 予測したトークンについて\n",
        "print('Total Predicted Tokens')\n",
        "print('total: {}'.format(len(cnt)))\n",
        "for k, v in cnt.items():\n",
        "  print(k,v)"
      ],
      "metadata": {
        "id": "fpecWXl3RHi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンと出現頻度の情報を CSV ファイルに出力する\n",
        "import csv\n",
        "\n",
        "with open(filepath+\"listtokencount.csv\", mode=\"w\", newline='') as f:\n",
        "  writer = csv.writer(f)\n",
        "  for d in cnt.most_common():\n",
        "    writer.writerow(d)\n"
      ],
      "metadata": {
        "id": "TwcCpUEOGMPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トークンの情報を画面に表示する\n",
        "for d in cnt.most_common():\n",
        "  print(d)"
      ],
      "metadata": {
        "id": "Fv9cZeWvHk1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers の Pipelines を利用した方法\n",
        "Hugging Face の Transformers に Pipelines ライブラリがあります．  \n",
        "こちらはモデルを使った推論が簡単にできるライブラリです．  \n",
        "書籍では紹介していませんが，ここで簡単に説明します．\n",
        "\n",
        "詳しい説明は [Hugging Face の説明](https://huggingface.co/docs/transformers/main_classes/pipelines) を参照してください．\n",
        "\n",
        "この Pipelines ライブラリの pipeline モジュールを使うことで MASK されたトークンの推定や文書分類や文書生成を Transformers にアップロードされている様々なモデルを使って実行することができます．\n",
        "\n",
        "モデルの内部構造まで踏み込んだ開発などではない際にモデルを使った処理をしたいという場合は便利なモジュールとなっています．"
      ],
      "metadata": {
        "id": "Wg5QG8vVPftq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# pipeline の準備\n",
        "# 今回は MASK されたトークンの推定として \"fill-mask\" を，\n",
        "# 使用するモデルとして \"bert-base-uncased\" を，\n",
        "# 上位から出力する個数として 10 (これは数値なので \"\" なし) を指定しています\n",
        "pipe = pipeline(\"fill-mask\", \"bert-base-uncased\", top_k=10)\n",
        "\n",
        "# MASK されたトークンの推定処理\n",
        "# 結果が result に出力されます\n",
        "# \"This is [MASK] . \" という文の [MASK] 部分に入るトークンを予測して出力が返ってきます．\n",
        "result = pipe(\"This is [MASK] .\")\n",
        "\n",
        "# 結果の表示\n",
        "# スコア (類似度)，トークン ID，トークン文字列，MASK を補完した文字列が出力されています\n",
        "print(f\"k: {len(result)}\")\n",
        "print(\"Predicted Tokens\")\n",
        "for t in result:\n",
        "  print(t)\n"
      ],
      "metadata": {
        "id": "hPB1KLfMmy_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記のプログラムの [MASK] 部分の推定をこの pipeline を利用した方法に置き換えることができます．"
      ],
      "metadata": {
        "id": "-iv3f1AGoQTU"
      }
    }
  ]
}